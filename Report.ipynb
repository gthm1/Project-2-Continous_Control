{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Continuous Control - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Overview Of The Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this project our goal is to train a agent i.e double jointed arm to move to target location. A reward of 0.1 is provided for every time step the arm is in the target location thus enabling the agent to maintain its position at the target location.\n",
    "We have 33 possible states corresponding to position, rotation, velocity, and angular velocities of the arm and action space is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Algorithm Explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Deterministic Policy Gradients :\n",
    "> DDPG uses four neural networks: a Q network, a deterministic policy network, a target Q network, and a target policy network.\n",
    "\n",
    "<img src='1.png'>\n",
    "\n",
    "> The Q network and policy network is very much like simple Advantage Actor-Critic, but in DDPG, the Actor directly maps states to actions (the output of the network directly the output) instead of outputting the probability distribution across a discrete action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improve stability in learning. Here’s why: In methods that do not use target networks, the update equations of the network are interdependent on the values calculated by the network itself, which makes it prone to divergence.\n",
    "\n",
    "<img src='2.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DDPG Algorithm:\n",
    "\n",
    "<img src='3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replay Buffer:\n",
    "> As used in Deep Q learning (and many other RL algorithms), DDPG also uses a replay buffer to sample experience to update neural network parameters. During each trajectory roll-out, we save all the experience tuples (state, action, reward, next_state) and store them in a finite-sized cache — a “replay buffer.” Then, we sample random mini-batches of experience from the replay buffer when we update the value and policy networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor (Policy) & Critic (Value) Network:\n",
    "> The value network is updated similarly as is done in Q-learning. The updated Q value is obtained by the Bellman equation:\n",
    "\n",
    "<img src='4.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, in DDPG, the next-state Q values are calculated with the target value network and target policy network. Then, we minimize the mean-squared loss between the updated Q value and the original Q value:\n",
    "<img src='5.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the policy function, our objective is to maximize the expected return:\n",
    "<img src='6.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To calculate the policy loss, we take the derivative of the objective function with respect to the policy parameter. Keep in mind that the actor (policy) function is differentiable, so we have to apply the chain rule.\n",
    "<img src='7.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:\n",
    "<img src='8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters Used :\n",
    "<p>\n",
    "OU_SIGMA = 0.2\n",
    "    \n",
    "OU_THETA = 0.15\n",
    "\n",
    "LEARN_EVERY = 20 \n",
    "\n",
    "LEARN_NUM = 10  \n",
    "\n",
    "GRAD_CLIPPING = 1.0  \n",
    "\n",
    "BUFFER_SIZE = int(1e6) \n",
    "\n",
    "BATCH_SIZE = 128   \n",
    "\n",
    "WEIGHT_DECAY = 0     \n",
    "\n",
    "EPSILON = 1.0        \n",
    "\n",
    "EPSILON_DECAY = 1e-6\n",
    "\n",
    "GAMMA = 0.99        \n",
    "\n",
    "TAU = 1e-3          \n",
    "\n",
    "LR_ACTOR = 1e-3      \n",
    "\n",
    "LR_CRITIC = 1e-3  \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Architecture:\n",
    "> Here we have two different networks for actor and critic. Below i have mentioned the architectures of the both networks.\n",
    "\n",
    "                Actor:\n",
    ">                   fc1(400 units) -- batch_normalization -- fc2(300 units) -- fc3(4 units)\n",
    "\n",
    "              Critic:\n",
    ">                   fc1(400 units) -- batch_normalization -- fc2(300 units + 4 units) -- fc3(1 units)\n",
    "\n",
    "> Here we have used relu as the activation function and adam as the optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "> It took 564 episodes to solve the environment with an Average score: 30.02\n",
    "<img src='9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas For Further Improvement:\n",
    "> Algorithms such as PPO,A3C can also be considered for these kind of problems.\n",
    "> Experiment with different values for hyperparameters such as fc1 units, fc2 units, batch_size etc.\n",
    "> Adding a few additional layers to the actor and critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
